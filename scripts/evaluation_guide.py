#!/usr/bin/env python3
"""
Model Evaluation Usage Guide

This script demonstrates how to use the model evaluation system
and shows different ways to evaluate your trained models.
"""

import json
from pathlib import Path

def show_evaluation_usage():
    """Show how to use the evaluation system."""
    
    print("üéØ MODEL EVALUATION SYSTEM USAGE GUIDE")
    print("="*60)
    
    print("\nüìã Available Evaluation Scripts:")
    print("="*40)
    
    print("\n1Ô∏è‚É£ Quick Evaluation (Recommended for fast testing)")
    print("   Command: python quick_evaluate.py")
    print("   Features:")
    print("   ‚Ä¢ Fast evaluation of both SAE and RBM models")
    print("   ‚Ä¢ Key performance metrics")
    print("   ‚Ä¢ Model architecture and hyperparameter display")
    print("   ‚Ä¢ Loads saved models with best hyperparameters")
    
    print("\n2Ô∏è‚É£ Comprehensive Evaluation (Full analysis)")
    print("   Command: python evaluate_models.py --models sae rbm --visualize")
    print("   Features:")
    print("   ‚Ä¢ Detailed performance analysis")
    print("   ‚Ä¢ Visual dashboard with charts")
    print("   ‚Ä¢ Model comparison")
    print("   ‚Ä¢ Results saved to JSON file")
    print("   ‚Ä¢ Comprehensive metrics including precision, recall, NDCG")
    
    print("\n3Ô∏è‚É£ Individual Model Evaluation")
    print("   SAE only: python evaluate_models.py --models sae")
    print("   RBM only: python evaluate_models.py --models rbm")
    
    print("\nüìä Available Metrics:")
    print("="*30)
    
    print("\nüß† SAE (Stacked AutoEncoder) Metrics:")
    print("   ‚Ä¢ RMSE (Root Mean Square Error)")
    print("   ‚Ä¢ MAE (Mean Absolute Error)")
    print("   ‚Ä¢ R¬≤ Score (Coefficient of Determination)")
    print("   ‚Ä¢ Precision@K, Recall@K, NDCG@K")
    print("   ‚Ä¢ Catalog Coverage")
    
    print("\n‚ö° RBM (Restricted Boltzmann Machine) Metrics:")
    print("   ‚Ä¢ Reconstruction Error")
    print("   ‚Ä¢ Binary Accuracy")
    print("   ‚Ä¢ Free Energy Statistics")
    print("   ‚Ä¢ Average Free Energy")
    
    print("\nüìà Visualization Features:")
    print("="*35)
    print("   ‚Ä¢ Model architecture comparison")
    print("   ‚Ä¢ Parameter count comparison")
    print("   ‚Ä¢ Performance metrics charts")
    print("   ‚Ä¢ Hyperparameter displays")
    print("   ‚Ä¢ Training statistics")

def show_latest_results():
    """Show the latest evaluation results."""
    
    print("\nüèÜ LATEST EVALUATION RESULTS")
    print("="*40)
    
    results_file = Path("evaluation_results/evaluation_results.json")
    
    if not results_file.exists():
        print("‚ùå No evaluation results found. Run evaluation first:")
        print("   python quick_evaluate.py")
        return
    
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    print(f"\nüìÖ Evaluation Date: {results.get('evaluation_timestamp', 'Unknown')}")
    print(f"üìä Dataset: {results.get('dataset', 'Unknown')}")
    
    if 'sae' in results:
        sae = results['sae']
        print(f"\nüß† SAE Performance:")
        print(f"   Architecture: {sae.get('architecture', 'N/A')}")
        print(f"   Parameters: {sae.get('total_parameters', 'N/A'):,}")
        print(f"   RMSE: {sae.get('rmse', 'N/A'):.4f}")
        print(f"   MAE: {sae.get('mae', 'N/A'):.4f}")
        print(f"   R¬≤ Score: {sae.get('r2_score', 'N/A'):.4f}")
    
    if 'rbm' in results:
        rbm = results['rbm']
        print(f"\n‚ö° RBM Performance:")
        print(f"   Architecture: {rbm.get('architecture', 'N/A')}")
        print(f"   Parameters: {rbm.get('total_parameters', 'N/A'):,}")
        print(f"   Reconstruction Error: {rbm.get('reconstruction_error', 'N/A'):.4f}")
        print(f"   Binary Accuracy: {rbm.get('binary_accuracy', 'N/A'):.4f}")
    
    if 'comparison' in results:
        comp = results['comparison']
        print(f"\nüìä Model Comparison:")
        if 'parameter_efficiency' in comp:
            pe = comp['parameter_efficiency']
            print(f"   RBM has {pe.get('ratio', 'N/A'):.2f}x more parameters than SAE")

def show_performance_interpretation():
    """Show how to interpret the performance metrics."""
    
    print("\nüîç PERFORMANCE METRICS INTERPRETATION")
    print("="*50)
    
    print("\nüìè Rating Prediction Metrics (SAE):")
    print("   ‚Ä¢ RMSE (Root Mean Square Error):")
    print("     - Lower is better (0 = perfect)")
    print("     - Typical range: 0.8-1.2 for movie ratings")
    print("     - Heavily penalizes large errors")
    
    print("\n   ‚Ä¢ MAE (Mean Absolute Error):")
    print("     - Lower is better (0 = perfect)")
    print("     - More interpretable than RMSE")
    print("     - Average prediction error in rating points")
    
    print("\n   ‚Ä¢ R¬≤ Score (Coefficient of Determination):")
    print("     - Higher is better (1 = perfect, 0 = as good as mean)")
    print("     - Negative values = worse than predicting the mean")
    print("     - Measures how much variance is explained")
    
    print("\nüîÑ Reconstruction Metrics (RBM):")
    print("   ‚Ä¢ Reconstruction Error:")
    print("     - Lower is better")
    print("     - Measures how well the model reconstructs input")
    
    print("\n   ‚Ä¢ Binary Accuracy:")
    print("     - Higher is better (1.0 = 100% accurate)")
    print("     - Percentage of correctly predicted binary preferences")
    
    print("\nüéØ Recommendation Quality Metrics:")
    print("   ‚Ä¢ Precision@K: Proportion of relevant items in top-K")
    print("   ‚Ä¢ Recall@K: Proportion of relevant items found in top-K")
    print("   ‚Ä¢ NDCG@K: Normalized Discounted Cumulative Gain")
    print("   ‚Ä¢ Catalog Coverage: Diversity of recommended items")

def main():
    """Main function."""
    
    show_evaluation_usage()
    show_latest_results()
    show_performance_interpretation()
    
    print(f"\n‚ú® QUICK START:")
    print("="*20)
    print("1. Run quick evaluation: python quick_evaluate.py")
    print("2. Run full evaluation: python evaluate_models.py --visualize")
    print("3. Check results in: evaluation_results/")
    
    print(f"\nüìù Notes:")
    print("‚Ä¢ Models are automatically loaded with their best hyperparameters")
    print("‚Ä¢ Evaluation uses the test set from training")
    print("‚Ä¢ Results are saved for future reference")
    print("‚Ä¢ Visualizations help compare model performance")

if __name__ == "__main__":
    main()
